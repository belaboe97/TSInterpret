{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETS\n",
    "\n",
    "A counterfactual explanation, originally introduced to machine learning by [1], answers the question \"what if\" by building counterexamples. Based on an input instance $x$, the goal is to find a counterfactual $x^{cf}$ close to the original instance $x$ but differently classified $y \\neq y^{cf}$ by a predictor $f$. The intention is to visualize boundary cases. Further research has shown that counterfactual explanations are easy to understand for humans because they are intuitive  to human thinking by showing counterexamples.  \n",
    "\n",
    "Shapelet-based Temporal Association Rule Mining for Multivariate Time Series Classification developed by Bahri et al. [1] builds counterfactuals in multivariate setting by perturbing the features of a time series with the help of a shapelet algorithm. Hereby they extract the most prominent class shapelets using an adaption of the shapelet transform algorithm of [2] implemented in sktime. The algorithm is adapted to multivariate timeseries analysis, treating each dimension as a univariate timeseries and information gain. The algorithm imputes the shapelets based on prior observed occcurences and shapelet quality. If a single shapelet does not cause a counterfactual prediction, more perturbations in other dimensions or other parts of the timeseries are performed. \n",
    "\n",
    "\n",
    "<img src=\"https://fzi-forschungszentrum-informatik.github.io/TSInterpret/Notebooks/SETS_CF.png\" alt=\"Visualization of SETS_CF\" style=\"width: 800px;\" />\n",
    "\n",
    "\n",
    "\n",
    "Visualization of SETS from the original paper [1]; The code in TSInterpret is based on the authors <a href='https://github.com/omarbahri/SETS'>implementation </a>.\n",
    "\n",
    "[1] Omar Bahri and Soukaina Filali Boubrahimi and Shah Muhammad Hamdi.Shapelet-Based Counterfactual Explanations for Multivariate Time Series. 2022 ACM SIGKDD Workshop on Mining and Learning from Time Series. arXiv:2208.10462 URL:https://arxiv.org/abs/2208.10462 (visited on 2023-12-04)\n",
    "\n",
    "\n",
    "[2] Hills, Jon and Lines, Jason and Baranauskas, Edgaras and Mapp, James and Bagnall, Anthony. Classification of time series by shapelet transformation. Data Mining and Knowledge Discovery. Harv. JL & Tech. 31 (2017): 841."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import random\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pymop import Problem\n",
    "import os\n",
    "from tslearn.datasets import UCR_UEA_datasets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Model\n",
    "- Load Data and reshape the data to fit a 1D-Conv Convolutional Neural Network (CNN). Note that the input for a 1D-Conv CNN hat the shape (batch, timesteps, features).\n",
    "- Load Model with Pretrained Weigths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='GunPoint'\n",
    "X_train,train_y,X_test,test_y=UCR_UEA_datasets().load_dataset(dataset)\n",
    "train_x=np.swapaxes(X_train,1,2)#.reshape(-1,X_train.shape[-1],X_train.shape[-2])\n",
    "test_x=np.swapaxes(X_test,1,2)#.reshape(-1,X_train.shape[-1],X_train.shape[-2])\n",
    "#enc1=pickle.load(open(f'../../ClassificationModels//models/{dataset}/OneHotEncoder.pkl','rb'))\n",
    "#enc1=sklearn.preprocessing.OneHotEncoder(sparse=False).fit(np.vstack((train_y.reshape(-1,1),test_y.reshape(-1,1))))\n",
    "#test_y=enc1.transform(test_y.reshape(-1,1))\n",
    "#train_y=enc1.transform(train_y.reshape(-1,1))\n",
    "n_classes = len(np.unique(test_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ClassificationModels.CNN_T import ResNetBaseline, UCRDataset, get_all_preds, fit\n",
    "stride = 1\n",
    "kernel_size=10\n",
    "padding = kernel_size - 1\n",
    "input_size= train_x.shape[-1]\n",
    "device = torch.device( \"cpu\")#\"cuda:0\" if torch.cuda.is_available() else\n",
    "model = ResNetBaseline(in_channels= 1, num_pred_classes=n_classes)\n",
    "\n",
    "model.load_state_dict(torch.load(f'../../ClassificationModels/models/{dataset}/ResNet'))\n",
    "#torch.save(model.state_dict(), f'../../ClassificationModels/models/{dataset}/ResNet')\n",
    "test_dataset = UCRDataset(test_x,test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=True)\n",
    "model.eval()\n",
    "y_pred,labels= get_all_preds(model,test_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability Algorithm\n",
    "\n",
    "Using a interpretability algorithm consists of 4 steps:\n",
    "\n",
    "1. Load the Interpretability Method\n",
    "2. Instaniate the Method with the desired Parameters\n",
    "3. Call the explain Method\n",
    "4. Plot the results\n",
    "\n",
    "### 1. & 2. Loading & Initialization\n",
    "\n",
    "SETS works on all models returning a probability function. The Initialization takes the following arguments:\n",
    "\n",
    "    `model`: The model to be explaines.\n",
    "    `data`: Tuple of Data and Labels.\n",
    "    `backend`: `PYT`, `SK`, or `TF`.\n",
    "    `mode`: second dimension is either `feat` or `time`. \n",
    "    `method`: Optimization Method either `brut` or `opt`.\n",
    "    `min_shapelet_len` : Value for min length of extracted shapelets \n",
    "    `max_shapelet_len`: Value for max length of extracted shapelets\n",
    "    `time_contract_in_mins_per_dim` : Max time for shapelet extraction per dimension\n",
    "    `initial_num_shapelets_per_case` : Initial number of shapelets per case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Univariate ECG200 Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1, 150)\n",
      "(50, 1)\n",
      "Extract Shapelets with information gain rejection lvl 0.001 and shapelets per class of 30\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from TSInterpret.InterpretabilityModels.counterfactual.SETSCF import SETSCF\n",
    "\n",
    "#train_x, train_y, test_x, test_y = X_train,train_y,X_test,test_y\n",
    "\n",
    "# Note: this is for demonstration purposes, time contract per minutes should be adjusted for better results! \n",
    "exp_model= SETSCF(model,\n",
    "                  (train_x, train_y),\n",
    "                  backend='PYT',\n",
    "                  mode='feat',  \n",
    "                  min_shapelet_len=3,\n",
    "                  max_shapelet_len=20,\n",
    "                  time_contract_in_mins_per_dim=1,\n",
    "                  #initial_num_shapelets_per_case=10,\n",
    "                  fit_shapelets = False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1, 150)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Call the fit method.\n",
    "\n",
    "This method is optional! If shapelets are already exctracted, please specify here the occlusion threshhold and if shapelets belonging to multiple classes should be retained. \n",
    "This method is called by default if the explain_model is fitted. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit function to prune shapelets with occlusion threshhold of 0.1 and remove shapelets belonging to more than one class set to True\n",
      "Shapelet by index per class and dimension: {0: [[0, 1, 3, 5, 7, 8, 11, 13, 15, 18]], 1: [[2, 4, 6, 9, 10, 12, 14, 16, 17]]}\n"
     ]
    }
   ],
   "source": [
    "exp_model.fit(occlusion_threshhold=1e-1,remove_multiclass_shapelets=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Call the explain method.\n",
    "\n",
    "Prepeare the instance and the predicted label of the instance as parameters for the explain methods.\n",
    "\n",
    "- `item`: item to be explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts 0\n",
      "{0: KNeighborsTimeSeries(n_neighbors=1), 1: KNeighborsTimeSeries(n_neighbors=1)}\n",
      "0\n",
      "Could not find a cf for this timeseries\n"
     ]
    }
   ],
   "source": [
    "ts = 1\n",
    "test_y[ts]\n",
    "print('ts',y_pred[ts])\n",
    "cf_explanation, label = exp_model.explain(test_x[ts], target = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualization\n",
    "\n",
    "All plot function take as input the item to be explained and the returned explanation. As as additonal option a figsize can be given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43mts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mts\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcf_explanation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvis_change\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mall_in_one\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_fig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/snap/code/174/.local/share/virtualenvs/TSInterpret-NXJYnQDU/lib/python3.10/site-packages/TSInterpret/InterpretabilityModels/counterfactual/CF.py:99\u001b[0m, in \u001b[0;36mCF.plot\u001b[0;34m(self, original, org_label, exp, exp_label, vis_change, all_in_one, save_fig, figsize)\u001b[0m\n\u001b[1;32m     97\u001b[0m ax011 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m ax012 \u001b[38;5;241m=\u001b[39m ax011\u001b[38;5;241m.\u001b[39mtwinx()\n\u001b[0;32m---> 99\u001b[0m sal_02 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\u001b[43moriginal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vis_change:\n\u001b[1;32m    103\u001b[0m     sns\u001b[38;5;241m.\u001b[39mheatmap(\n\u001b[1;32m    104\u001b[0m         sal_02,\n\u001b[1;32m    105\u001b[0m         fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m         yticklabels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    110\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'NoneType'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADZCAYAAAAjWzSXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAaZ0lEQVR4nO3de2xUZf7H8U9bmCnGtmC6nZYyu11wFStI19bOFiTEzaxNNHX5Y2NXDO02XlbtGmWyK1QuA6KU9UKaSJXIevtDF9SIMdLU1a7EqN00W2giW8Bg0bbEGWhc6WzRKXSe3x+G8Td2wJ5KZ8qc9yuZP87j9znPt30s55MztzRjjBEAAIBNpSe7AQAAgGQiDAEAAFsjDAEAAFsjDAEAAFsjDAEAAFsjDAEAAFsjDAEAAFsjDAEAAFsjDAEAAFsjDAEAAFuzHIbef/99VVVVaebMmUpLS9Mbb7zxg3P27Nmjq6++Wk6nU5deeqleeOGFcbQKAABSWbIyhuUwNDQ0pAULFqi5uXlM9UeOHNGNN96o6667Tl1dXbr//vt1++236+2337bcLAAASF3JyhhpP+aLWtPS0rRr1y4tXbr0rDUrV67U7t27tX///ujY73//e3311VdqbW0d79IAACCFJTJjTPkxjY5Fe3u7vF5vzFhlZaXuv//+s84Jh8MKh8PR49OnT+vAgQNyu91KT+dlTgAAXAgikYh6e3tVXFysKVO+ixxOp1NOp/NHn388GSOeCQ9DgUBALpcrZszlcmlwcFBff/21pk2bNmpOY2OjNmzYMNGtAQCAJPD7/Vq/fv2PPs94MkY8Ex6GxqOhoUE+ny963NfXp3nz5qmjo0MFBQVJ7AwAAIzVF198ofLycu3fv19utzs6fj7uCp1PEx6G8vPzFQwGY8aCwaCys7PPmti+f/ssJydHklRQUKBZs2ZNXLMAAOC8y8nJUXZ29nk/73gyRjwT/gKciooKtbW1xYy98847qqiomOilAQBACjtfGcNyGPrf//6nrq4udXV1Sfr2bW1dXV3q7e2V9O1TXDU1NdH6u+66Sz09PXrggQd08OBBPfXUU3rllVe0YsUKq0sDAIAUlrSMYSx67733jKRRj9raWmOMMbW1tWbJkiWj5pSUlBiHw2Fmz55tnn/+eUtr9vX1GUmmr6/ParsAACBJrF6/k5ExjDHmR33OUKL09/fL7Xarr6+P1wwBAHCBuFCu33xoDwAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsLVxhaHm5mYVFRUpMzNTHo9HHR0d56xvamrS5ZdfrmnTpsntdmvFihX65ptvxtUwAABIXcnIGJbD0M6dO+Xz+eT3+7V3714tWLBAlZWVOnbsWNz6l19+WatWrZLf79eBAwf07LPPaufOnXrwwQetLg0AAFJYsjKG5TC0ZcsW3XHHHaqrq1NxcbG2bdumiy66SM8991zc+o8++kiLFi3SsmXLVFRUpOuvv1633HLLDyY9AABgL8nKGJbC0PDwsDo7O+X1er87QXq6vF6v2tvb485ZuHChOjs7o4319PSopaVFN9xww1nXCYfDGhwcjD5CoZCVNgEAwCQSCoViruvhcHhUTaIyRjxTrBQPDAxoZGRELpcrZtzlcungwYNx5yxbtkwDAwO69tprZYzR6dOnddddd53zFlZjY6M2bNhgpTUAADBJFRcXxxz7/X6tX78+ZixRGSOeCX832Z49e7Rp0yY99dRT2rt3r15//XXt3r1bGzduPOuchoYGnThxIvro7u6e6DYBAMAE6e7ujrmuNzQ0nJfzjidjxGPpzlBubq4yMjIUDAZjxoPBoPLz8+POWbt2rZYvX67bb79dkjR//nwNDQ3pzjvv1OrVq5WePjqPOZ1OOZ3O6PHg4KCVNgEAwCSSlZWl7Ozsc9YkKmPEY+nOkMPhUGlpqdra2qJjkUhEbW1tqqioiDvn5MmTo5rJyMiQJBljrCwPAABSVDIzhqU7Q5Lk8/lUW1ursrIylZeXq6mpSUNDQ6qrq5Mk1dTUqLCwUI2NjZKkqqoqbdmyRb/85S/l8Xh0+PBhrV27VlVVVdGGAQAAkpUxLIeh6upqHT9+XOvWrVMgEFBJSYlaW1ujL3jq7e2NSWlr1qxRWlqa1qxZo6NHj+onP/mJqqqq9Mgjj1hdGgAApLBkZYw0cwE8V9Xf3y+3262+vj7NmjUr2e0AAIAxuFCu33w3GQAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsDXCEAAAsLVxhaHm5mYVFRUpMzNTHo9HHR0d56z/6quvVF9fr4KCAjmdTl122WVqaWkZV8MAACB1JSNjTLHa5M6dO+Xz+bRt2zZ5PB41NTWpsrJShw4dUl5e3qj64eFh/eY3v1FeXp5ee+01FRYW6vPPP9f06dOtLg0AAFJYsjJGmjHGWJng8Xh0zTXXaOvWrZKkSCQit9ute++9V6tWrRpVv23bNj322GM6ePCgpk6daqm5M/r7++V2u9XX16dZs2aN6xwAACCxrF6/k5ExJItPkw0PD6uzs1Ner/e7E6Sny+v1qr29Pe6cN998UxUVFaqvr5fL5dK8efO0adMmjYyMnHWdcDiswcHB6CMUCllpEwAATCKhUCjmuh4Oh0fVJCpjxGMpDA0MDGhkZEQulytm3OVyKRAIxJ3T09Oj1157TSMjI2ppadHatWv1xBNP6OGHHz7rOo2NjcrJyYk+iouLrbQJAAAmkeLi4pjremNj46iaRGWMeCy/ZsiqSCSivLw8PfPMM8rIyFBpaamOHj2qxx57TH6/P+6choYG+Xy+6PHRo0cJRAAAXKC6u7tVWFgYPXY6neflvOPJGPFYCkO5ubnKyMhQMBiMGQ8Gg8rPz487p6CgQFOnTlVGRkZ07IorrlAgENDw8LAcDseoOU6nM+YXNTg4aKVNAAAwiWRlZSk7O/ucNYnKGPFYeprM4XCotLRUbW1t0bFIJKK2tjZVVFTEnbNo0SIdPnxYkUgkOvbJJ5+ooKBgzE0CAIDUlsyMYflzhnw+n7Zv364XX3xRBw4c0N13362hoSHV1dVJkmpqatTQ0BCtv/vuu/Xll1/qvvvu0yeffKLdu3dr06ZNqq+vt7o0AABIYcnKGJZfM1RdXa3jx49r3bp1CgQCKikpUWtra/QFT729vUpP/y5jud1uvf3221qxYoWuuuoqFRYW6r777tPKlSutLg0AAFJYsjKG5c8ZSgY+ZwgAgAvPhXL95rvJAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArRGGAACArY0rDDU3N6uoqEiZmZnyeDzq6OgY07wdO3YoLS1NS5cuHc+yAAAgxSUjY1gOQzt37pTP55Pf79fevXu1YMECVVZW6tixY+ec99lnn+nPf/6zFi9ebLlJAACQ+pKVMSyHoS1btuiOO+5QXV2diouLtW3bNl100UV67rnnzjpnZGREt956qzZs2KDZs2ePq1EAAJDakpUxLIWh4eFhdXZ2yuv1fneC9HR5vV61t7efdd5DDz2kvLw83XbbbWNaJxwOa3BwMPoIhUJW2gQAAJNIKBSKua6Hw+FRNYnKGPFYCkMDAwMaGRmRy+WKGXe5XAoEAnHnfPDBB3r22We1ffv2Ma/T2NionJyc6KO4uNhKmwAAYBIpLi6Oua43NjaOqklUxohnyo+a/QNCoZCWL1+u7du3Kzc3d8zzGhoa5PP5osdHjx4lEAEAcIHq7u5WYWFh9NjpdP7oc443Y8RjKQzl5uYqIyNDwWAwZjwYDCo/P39U/aeffqrPPvtMVVVV0bFIJPLtwlOm6NChQ5ozZ86oeU6nM+YXNTg4aKVNAAAwiWRlZSk7O/ucNYnKGPFYeprM4XCotLRUbW1tMQu3tbWpoqJiVP3cuXP18ccfq6urK/q46aabdN1116mrq0tut9vK8gAAIEUlM2NYfprM5/OptrZWZWVlKi8vV1NTk4aGhlRXVydJqqmpUWFhoRobG5WZmal58+bFzJ8+fbokjRoHAAD2lqyMYTkMVVdX6/jx41q3bp0CgYBKSkrU2toafcFTb2+v0tP5YGsAAGBNsjJGmjHGnPeznmf9/f1yu93q6+vTrFmzkt0OAAAYgwvl+s0tHAAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGuEIQAAYGvjCkPNzc0qKipSZmamPB6POjo6zlq7fft2LV68WDNmzNCMGTPk9XrPWQ8AAOwrGRnDchjauXOnfD6f/H6/9u7dqwULFqiyslLHjh2LW79nzx7dcssteu+999Te3i63263rr79eR48etdwsAABIXcnKGGnGGGNlgsfj0TXXXKOtW7dKkiKRiNxut+69916tWrXqB+ePjIxoxowZ2rp1q2pqasa0Zn9/v9xut/r6+jRr1iwr7QIAgCSxev1ORsaQLN4ZGh4eVmdnp7xe73cnSE+X1+tVe3v7mM5x8uRJnTp1SpdccomVpQEAQApLZsaYYqV4YGBAIyMjcrlcMeMul0sHDx4c0zlWrlypmTNnxvyw3xcOhxUOh6PHoVDISpsAAGASCYVCGhwcjB47nU45nc6YmkRljHgS+m6yzZs3a8eOHdq1a5cyMzPPWtfY2KicnJzoo7i4OIFdAgCA86m4uDjmut7Y2Hje1xhrxojH0p2h3NxcZWRkKBgMxowHg0Hl5+efc+7jjz+uzZs3691339VVV111ztqGhgb5fL7o8dGjRwlEAABcoLq7u1VYWBg9/v5dISlxGSMeS3eGHA6HSktL1dbWFh2LRCJqa2tTRUXFWec9+uij2rhxo1pbW1VWVvaD6zidTmVnZ0cfWVlZVtoEAACTSFZWVsx1PV4YSlTGiMfSnSFJ8vl8qq2tVVlZmcrLy9XU1KShoSHV1dVJkmpqalRYWBi9BfbXv/5V69at08svv6yioiIFAgFJ0sUXX6yLL754XE0DAIDUk6yMYTkMVVdX6/jx41q3bp0CgYBKSkrU2toafcFTb2+v0tO/u+H09NNPa3h4WL/73e9izuP3+7V+/XqrywMAgBSVrIxh+XOGkoHPGQIA4MJzoVy/+W4yAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga4QhAABga+MKQ83NzSoqKlJmZqY8Ho86OjrOWf/qq69q7ty5yszM1Pz589XS0jKuZgEAQGpLRsawHIZ27twpn88nv9+vvXv3asGCBaqsrNSxY8fi1n/00Ue65ZZbdNttt2nfvn1aunSpli5dqv3791tuFgAApK5kZYw0Y4yxMsHj8eiaa67R1q1bJUmRSERut1v33nuvVq1aNaq+urpaQ0NDeuutt6Jjv/rVr1RSUqJt27aNac3+/n653W719fVp1qxZVtoFAABJYvX6nYyMIUlTxlwpaXh4WJ2dnWpoaIiOpaeny+v1qr29Pe6c9vZ2+Xy+mLHKykq98cYbZ10nHA4rHA5Hj0+cOCFJ+uKLL6y0CwAAkujMdfvEiRPKzs6OjjudTjmdzpjaRGWMeCyFoYGBAY2MjMjlcsWMu1wuHTx4MO6cQCAQtz4QCJx1ncbGRm3YsGHUeHl5uZV2AQDAJDBv3ryYY7/fr/Xr18eMJSpjxGMpDCVKQ0NDTNL78ssv9fOf/1z79+9XTk5OEjtDKBRScXGxuru7lZWVlex2bI29mDzYi8mF/Zg8Tpw4oXnz5unIkSO65JJLouPfvyuUbJbCUG5urjIyMhQMBmPGg8Gg8vPz487Jz8+3VC/Fv30mSW63O+Y2GxJvcHBQklRYWMheJBl7MXmwF5ML+zF5nPn9X3LJJT+4F4nKGPFYejeZw+FQaWmp2traomORSERtbW2qqKiIO6eioiKmXpLeeeeds9YDAAD7SWbGsPw0mc/nU21trcrKylReXq6mpiYNDQ2prq5OklRTU6PCwkI1NjZKku677z4tWbJETzzxhG688Ubt2LFD//73v/XMM89YXRoAAKSwZGUMy2Gourpax48f17p16xQIBFRSUqLW1tboC5h6e3uVnv7dDaeFCxfq5Zdf1po1a/Tggw/qF7/4hd54441RL6Y6F6fTKb/fP+meY7Qj9mLyYC8mD/ZicmE/Jg+re5GMjCGN43OGAAAAUgnfTQYAAGyNMAQAAGyNMAQAAGyNMAQAAGxt0oSh5uZmFRUVKTMzUx6PRx0dHeesf/XVVzV37lxlZmZq/vz5amlpSVCnqc/KXmzfvl2LFy/WjBkzNGPGDHm93h/cO4yd1b+LM3bs2KG0tDQtXbp0Yhu0Eat78dVXX6m+vl4FBQVyOp267LLL+HfqPLG6F01NTbr88ss1bdo0ud1urVixQt98802Cuk1d77//vqqqqjRz5kylpaWN6fvA9uzZo6uvvlpOp1OXXnqpXnjhhQnvc0zMJLBjxw7jcDjMc889Z/7zn/+YO+64w0yfPt0Eg8G49R9++KHJyMgwjz76qOnu7jZr1qwxU6dONR9//HGCO089Vvdi2bJlprm52ezbt88cOHDA/OEPfzA5OTmmv78/wZ2nHqt7ccaRI0dMYWGhWbx4sfntb3+bmGZTnNW9CIfDpqyszNxwww3mgw8+MEeOHDF79uwxXV1dCe489Vjdi5deesk4nU7z0ksvmSNHjpi3337bFBQUmBUrViS489TT0tJiVq9ebV5//XUjyezateuc9T09Peaiiy4yPp/PdHd3myeffNJkZGSY1tbWxDR8DpMiDJWXl5v6+vro8cjIiJk5c6ZpbGyMW3/zzTebG2+8MWbM4/GYP/7xjxPapx1Y3YvvO336tMnKyjIvvvjiRLVoG+PZi9OnT5uFCxeav/3tb6a2tpYwdJ5Y3Yunn37azJ492wwPDyeqRduwuhf19fXm17/+dcyYz+czixYtmtA+7WYsYeiBBx4wV155ZcxYdXW1qaysnMDOxibpT5MNDw+rs7NTXq83Opaeni6v16v29va4c9rb22PqJamysvKs9Rib8ezF9508eVKnTp2K+UI+WDfevXjooYeUl5en2267LRFt2sJ49uLNN99URUWF6uvr5XK5NG/ePG3atEkjIyOJajsljWcvFi5cqM7OzuhTaT09PWppadENN9yQkJ7xncl87U76t9YPDAxoZGQk+umSZ7hcLh08eDDunEAgELc+EAhMWJ92MJ69+L6VK1dq5syZo/6HhzXj2YsPPvhAzz77rLq6uhLQoX2MZy96enr0z3/+U7feeqtaWlp0+PBh3XPPPTp16pT8fn8i2k5J49mLZcuWaWBgQNdee62MMTp9+rTuuusuPfjgg4loGf/P2a7dg4OD+vrrrzVt2rQkdTaJXkCNC9/mzZu1Y8cO7dq1S5mZmclux1ZCoZCWL1+u7du3Kzc3N9nt2F4kElFeXp6eeeYZlZaWqrq6WqtXr9a2bduS3Zrt7NmzR5s2bdJTTz2lvXv36vXXX9fu3bu1cePGZLeGSSTpd4Zyc3OVkZGhYDAYMx4MBpWfnx93Tn5+vqV6jM149uKMxx9/XJs3b9a7776rq666aiLbtAWre/Hpp5/qs88+U1VVVXQsEolIkqZMmaJDhw5pzpw5E9t0ihrP30VBQYGmTp2qjIyM6NgVV1yhQCCg4eFhORyOCe05VY1nL9auXavly5fr9ttvlyTNnz9fQ0NDuvPOO7V69eqY77nCxDrbtTs7Ozupd4WkSXBnyOFwqLS0VG1tbdGxSCSitrY2VVRUxJ1TUVERUy9J77zzzlnrMTbj2QtJevTRR7Vx40a1traqrKwsEa2mPKt7MXfuXH388cfq6uqKPm666SZdd9116urqktvtTmT7KWU8fxeLFi3S4cOHo4FUkj755BMVFBQQhH6E8ezFyZMnRwWeMyHV8NWcCTWpr93JfgW3Md++VdLpdJoXXnjBdHd3mzvvvNNMnz7dBAIBY4wxy5cvN6tWrYrWf/jhh2bKlCnm8ccfNwcOHDB+v5+31p8nVvdi8+bNxuFwmNdee8188cUX0UcoFErWj5AyrO7F9/FusvPH6l709vaarKws86c//ckcOnTIvPXWWyYvL888/PDDyfoRUobVvfD7/SYrK8v8/e9/Nz09PeYf//iHmTNnjrn55puT9SOkjFAoZPbt22f27dtnJJktW7aYffv2mc8//9wYY8yqVavM8uXLo/Vn3lr/l7/8xRw4cMA0Nzfz1vrve/LJJ81Pf/pT43A4THl5ufnXv/4V/W9LliwxtbW1MfWvvPKKueyyy4zD4TBXXnml2b17d4I7Tl1W9uJnP/uZkTTq4ff7E994CrL6d/H/EYbOL6t78dFHHxmPx2OcTqeZPXu2eeSRR8zp06cT3HVqsrIXp06dMuvXrzdz5swxmZmZxu12m3vuucf897//TXzjKea9996L++//md9/bW2tWbJkyag5JSUlxuFwmNmzZ5vnn38+4X3Hk2YM9wkBAIB9Jf01QwAAAMlEGAIAALZGGAIAALZGGAIAALZGGAIAALZGGAIAALZGGAIAALZGGAIAALZGGAIAALZGGAIAALZGGAIAALZGGAIAALb2f6kBjZXW50A7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_model.plot(\n",
    "        test_x[ts],\n",
    "        y_pred[ts],\n",
    "        cf_explanation,\n",
    "        label,\n",
    "        vis_change=True,\n",
    "        all_in_one=False,\n",
    "        save_fig=None,\n",
    "        figsize=(6.4, 4.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "dataset='BasicMotions'\n",
    "X_train,train_y,X_test,test_y=UCR_UEA_datasets().load_dataset(dataset)\n",
    "train_x=np.swapaxes(X_train,2,1)\n",
    "test_x=np.swapaxes(X_test,2,1)\n",
    "\n",
    "\n",
    "n_classes = len(np.unique(test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ClassificationModels.CNN_T import ResNetBaseline, UCRDataset, get_all_preds\n",
    "from ClassificationModels.LSTM_T import LSTM\n",
    "\n",
    "#device = torch.device( \"cpu\")#\"cuda:0\" if torch.cuda.is_available() else\n",
    "model = ResNetBaseline(in_channels=train_x.shape[1], num_pred_classes=n_classes)\n",
    "model.load_state_dict(torch.load(f'../../ClassificationModels//models/{dataset}/ResNet'))\n",
    "test_dataset = UCRDataset(test_x,test_y)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model(torch.from_numpy(test_x).float()).detach().numpy(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetBaseline(\n",
       "  (layers): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(6, 64, kernel_size=(8,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(64, 64, kernel_size=(5,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (2): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(64, 64, kernel_size=(3,), stride=(1,))\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual): Sequential(\n",
       "        (0): Conv1dSamePadding(6, 64, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(64, 128, kernel_size=(8,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(128, 128, kernel_size=(5,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (2): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(128, 128, kernel_size=(3,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual): Sequential(\n",
       "        (0): Conv1dSamePadding(64, 128, kernel_size=(1,), stride=(1,))\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ResNetBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(128, 128, kernel_size=(8,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(128, 128, kernel_size=(5,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (2): ConvBlock(\n",
       "          (layers): Sequential(\n",
       "            (0): Conv1dSamePadding(128, 128, kernel_size=(3,), stride=(1,))\n",
       "            (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model(torch.from_numpy(train_x).float()).detach().numpy()\n",
    "train_y=np.argmax(y_pred,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(40, 6, 100)\n",
      "(40, 6)\n",
      "Extract Shapelets with information gain rejection lvl 0.001 and shapelets per class of 30\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from TSInterpret.InterpretabilityModels.counterfactual.SETSCF import SETSCF\n",
    "exp_model= SETSCF(model,\n",
    "                  (train_x,train_y),\n",
    "                  backend='PYT',\n",
    "                  mode='feat',         \n",
    "                  min_shapelet_len=3,\n",
    "                  max_shapelet_len=20,\n",
    "                  time_contract_in_mins_per_dim=1,\n",
    "                  #initial_num_shapelets_per_case=10,\n",
    "                  fit_shapelets = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit function to prune shapelets with occlusion threshhold of 0.3 and remove shapelets belonging to more than one class set to False\n",
      "Shapelet by index per class and dimension: {0: [[0, 1, 18, 19, 23, 24, 25, 28, 29, 30, 31, 34, 35, 36, 39, 41, 42, 44, 50, 60, 65, 67], [0, 1, 2, 26, 29, 30, 32, 33, 35, 36, 42, 44, 48, 49, 54], [0, 5, 8, 15, 16, 18, 20, 21, 32, 34, 36, 41, 43, 50, 55], [10, 11, 18, 22, 23, 24, 28, 31, 33, 34, 40, 54, 55], [3, 8, 10, 14, 19, 20, 21, 22, 25, 36, 37, 40, 43], [0, 1, 2, 3, 4, 5, 20, 23, 24, 28, 29, 32]], 1: [[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 26, 58], [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 31, 50], [12, 19, 22, 23, 25, 31, 42, 44, 45, 48, 52, 53, 56, 57], [0, 2, 3, 4, 5, 7, 13, 14, 17, 19, 26, 29, 44], [0, 4, 5, 9, 11, 13, 15, 23, 30, 38, 42, 45], [6, 7, 8, 15, 16, 17, 21, 26, 27, 31]], 2: [[43, 45, 46, 47, 48, 49, 51, 52, 54, 55, 56, 57, 59, 61, 63], [34, 37, 38, 39, 43, 45, 46, 47, 51, 53, 55, 56, 57, 59], [9, 13, 26, 28, 29, 33, 38, 46, 47, 49, 54], [30, 32, 35, 36, 37, 38, 45, 49, 50, 51, 53], [12, 16, 17, 26, 28, 31, 32, 33, 34], [30, 33, 34, 35, 37, 38, 40]], 3: [[15, 16, 17, 20, 21, 22, 27, 32, 33, 37, 38, 40, 62, 66], [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28], [1, 2, 3, 4, 6, 7, 10, 14, 17, 27, 35], [1, 6, 8, 9, 12, 15, 16, 20, 21, 25, 27], [1, 2, 6, 7, 18, 24, 27, 29, 35, 41], [9, 10, 11, 12, 13, 14, 18, 19, 22]]}\n"
     ]
    }
   ],
   "source": [
    "exp_model.fit(occlusion_threshhold=3e-1,remove_multiclass_shapelets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Could not find a cf for this timeseries\n"
     ]
    }
   ],
   "source": [
    "ts = 4\n",
    "cf_explanation, label = exp_model.explain(train_x[ts]\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items are identical\n"
     ]
    }
   ],
   "source": [
    "#item, org_label, exp, cf_label, save_fig=None, figsize=(6.4, 4.8)\n",
    "exp_model.plot_in_one(train_x[ts],np.argmax(y_pred[ts]),cf_explanation,label,figsize=(15,15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSInterpret-NXJYnQDU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
